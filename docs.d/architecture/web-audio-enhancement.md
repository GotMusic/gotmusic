---
id: WEB_AUDIO_ENHANCEMENT
status: üî¥ CRITICAL
owner: @grantedwards
updated: 2025-10-23
priority: P0
---

# Web Audio Player Enhancement

**üî¥ CRITICAL** - Enhanced web audio player with Web Audio API, Canvas waveform visualization, and LUFS normalization.

---

## üéØ **OVERVIEW**

The Web Audio Player Enhancement transforms the basic HTML5 audio player into a **professional-grade audio player** with waveform visualization, LUFS normalization, and quality management. This is the **core web implementation** for the hackathon timeline.

### **Key Features**
- **üéµ Web Audio API**: High-performance audio processing
- **üìä Canvas Waveform**: Real-time waveform visualization
- **üéöÔ∏è LUFS Normalization**: Professional loudness normalization
- **‚öôÔ∏è Quality Settings**: Multiple bitrate options
- **üé® Professional UI**: Design system integration

---

## üèóÔ∏è **CURRENT STATE ANALYSIS**

### **Current Implementation**
```typescript
// Current: Basic HTML5 Audio
<audio ref={audioRef} src={src} preload="metadata" />
```

### **Limitations**
- ‚ùå No waveform visualization
- ‚ùå Basic audio quality (no LUFS normalization)
- ‚ùå Limited streaming capabilities
- ‚ùå No real-time audio processing

---

## üöÄ **ENHANCED IMPLEMENTATION**

### **Web Audio API Integration**

```typescript
// Enhanced Web Audio Player
interface EnhancedWebPlayer {
  // Audio Processing
  audioContext: AudioContext;
  gainNode: GainNode;
  analyserNode: AnalyserNode;
  sourceNode: AudioBufferSourceNode;
  
  // Waveform Visualization
  canvas: HTMLCanvasElement;
  waveformData: Float32Array;
  animationFrame: number;
  
  // Quality Control
  lufsNormalizer: LUFSNormalizer;
  qualityManager: QualityManager;
  
  // State Management
  isPlaying: boolean;
  position: number;
  duration: number;
  volume: number;
}

class WebAudioPlayer {
  private audioContext: AudioContext;
  private gainNode: GainNode;
  private analyserNode: AnalyserNode;
  private sourceNode: AudioBufferSourceNode | null = null;
  private audioBuffer: AudioBuffer | null = null;
  
  private canvas: HTMLCanvasElement;
  private ctx: CanvasRenderingContext2D;
  private animationFrame: number | null = null;
  
  private lufsNormalizer: LUFSNormalizer;
  private qualityManager: QualityManager;
  
  constructor(canvas: HTMLCanvasElement) {
    this.canvas = canvas;
    this.ctx = canvas.getContext('2d')!;
    
    // Initialize Web Audio API
    this.audioContext = new AudioContext();
    this.gainNode = this.audioContext.createGain();
    this.analyserNode = this.audioContext.createAnalyser();
    
    // Configure analyser
    this.analyserNode.fftSize = 2048;
    this.analyserNode.smoothingTimeConstant = 0.8;
    
    // Connect audio nodes
    this.gainNode.connect(this.analyserNode);
    this.analyserNode.connect(this.audioContext.destination);
    
    // Initialize processors
    this.lufsNormalizer = new LUFSNormalizer();
    this.qualityManager = new QualityManager();
  }
  
  async loadAudio(audioUrl: string): Promise<void> {
    try {
      // Fetch audio data
      const response = await fetch(audioUrl);
      const arrayBuffer = await response.arrayBuffer();
      
      // Decode audio
      this.audioBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
      
      // Apply LUFS normalization
      const normalizedBuffer = this.lufsNormalizer.normalizeAudio(this.audioBuffer);
      
      // Create source node
      this.sourceNode = this.audioContext.createBufferSource();
      this.sourceNode.buffer = normalizedBuffer;
      this.sourceNode.connect(this.gainNode);
      
      // Start waveform visualization
      this.startWaveformVisualization();
      
    } catch (error) {
      console.error('Failed to load audio:', error);
      throw error;
    }
  }
  
  play(): void {
    if (this.sourceNode && this.audioContext.state === 'suspended') {
      this.audioContext.resume();
    }
    
    if (this.sourceNode) {
      this.sourceNode.start();
      this.startWaveformVisualization();
    }
  }
  
  pause(): void {
    if (this.sourceNode) {
      this.sourceNode.stop();
      this.stopWaveformVisualization();
    }
  }
  
  setVolume(volume: number): void {
    this.gainNode.gain.value = volume;
  }
  
  setQuality(quality: QualityLevel): void {
    this.qualityManager.setQuality(quality);
  }
  
  private startWaveformVisualization(): void {
    const draw = () => {
      if (!this.analyserNode) return;
      
      const bufferLength = this.analyserNode.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);
      this.analyserNode.getByteTimeDomainData(dataArray);
      
      // Clear canvas
      this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
      
      // Draw waveform
      this.ctx.beginPath();
      this.ctx.strokeStyle = '#666';
      this.ctx.lineWidth = 1;
      
      const sliceWidth = this.canvas.width / bufferLength;
      let x = 0;
      
      for (let i = 0; i < bufferLength; i++) {
        const v = dataArray[i] / 128.0;
        const y = v * this.canvas.height / 2;
        
        if (i === 0) {
          this.ctx.moveTo(x, y);
        } else {
          this.ctx.lineTo(x, y);
        }
        
        x += sliceWidth;
      }
      
      this.ctx.stroke();
      
      this.animationFrame = requestAnimationFrame(draw);
    };
    
    draw();
  }
  
  private stopWaveformVisualization(): void {
    if (this.animationFrame) {
      cancelAnimationFrame(this.animationFrame);
      this.animationFrame = null;
    }
  }
}
```

---

## üìä **WAVEFORM VISUALIZATION**

### **Canvas Waveform Component**

```typescript
// Canvas Waveform Component
interface WaveformProps {
  audioBuffer: AudioBuffer;
  width: number;
  height: number;
  progress: number;
  color?: string;
  backgroundColor?: string;
}

const CanvasWaveform: React.FC<WaveformProps> = ({
  audioBuffer,
  width,
  height,
  progress,
  color = '#666',
  backgroundColor = '#000'
}) => {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  
  useEffect(() => {
    const canvas = canvasRef.current;
    if (!canvas || !audioBuffer) return;
    
    const ctx = canvas.getContext('2d');
    if (!ctx) return;
    
    // Set canvas size
    canvas.width = width;
    canvas.height = height;
    
    // Clear canvas
    ctx.fillStyle = backgroundColor;
    ctx.fillRect(0, 0, width, height);
    
    // Draw waveform
    const samples = audioBuffer.getChannelData(0);
    const samplesPerPixel = samples.length / width;
    
    ctx.beginPath();
    ctx.strokeStyle = color;
    ctx.lineWidth = 1;
    
    for (let x = 0; x < width; x++) {
      const start = Math.floor(x * samplesPerPixel);
      const end = Math.floor((x + 1) * samplesPerPixel);
      const block = samples.slice(start, end);
      
      const rms = Math.sqrt(
        block.reduce((sum, sample) => sum + sample * sample, 0) / block.length
      );
      
      const y = height / 2 + (rms * height / 2);
      
      if (x === 0) {
        ctx.moveTo(x, y);
      } else {
        ctx.lineTo(x, y);
      }
    }
    
    ctx.stroke();
    
    // Draw progress
    if (progress > 0) {
      ctx.strokeStyle = '#00ff00';
      ctx.lineWidth = 2;
      ctx.beginPath();
      ctx.moveTo(0, height / 2);
      ctx.lineTo(progress * width, height / 2);
      ctx.stroke();
    }
  }, [audioBuffer, width, height, progress, color, backgroundColor]);
  
  return <canvas ref={canvasRef} width={width} height={height} />;
};
```

### **Real-time Waveform Analysis**

```typescript
// Real-time Waveform Analysis
class WaveformAnalyzer {
  private analyserNode: AnalyserNode;
  private dataArray: Uint8Array;
  private bufferLength: number;
  
  constructor(analyserNode: AnalyserNode) {
    this.analyserNode = analyserNode;
    this.bufferLength = analyserNode.frequencyBinCount;
    this.dataArray = new Uint8Array(this.bufferLength);
  }
  
  getWaveformData(): number[] {
    this.analyserNode.getByteTimeDomainData(this.dataArray);
    
    // Convert to normalized values
    const waveform: number[] = [];
    for (let i = 0; i < this.bufferLength; i++) {
      const normalized = (this.dataArray[i] - 128) / 128;
      waveform.push(normalized);
    }
    
    return waveform;
  }
  
  getFrequencyData(): number[] {
    this.analyserNode.getByteFrequencyData(this.dataArray);
    
    // Convert to normalized values
    const frequencies: number[] = [];
    for (let i = 0; i < this.bufferLength; i++) {
      const normalized = this.dataArray[i] / 255;
      frequencies.push(normalized);
    }
    
    return frequencies;
  }
}
```

---

## üéöÔ∏è **LUFS NORMALIZATION**

### **LUFS Normalizer Implementation**

```typescript
// LUFS Normalizer
class LUFSNormalizer {
  private targetLUFS = -14.0; // Spotify standard
  private analysisWindow = 400; // ms
  private smoothingTime = 100; // ms
  
  public normalizeAudio(audioBuffer: AudioBuffer): AudioBuffer {
    // Get channel data
    const channelData = audioBuffer.getChannelData(0);
    
    // Calculate current LUFS
    const currentLUFS = this.analyzeLUFS(channelData);
    
    // Calculate gain adjustment
    const gainAdjustment = this.targetLUFS - currentLUFS;
    const gainLinear = Math.pow(10, gainAdjustment / 20);
    
    // Apply gain
    const normalizedData = new Float32Array(channelData.length);
    for (let i = 0; i < channelData.length; i++) {
      normalizedData[i] = channelData[i] * gainLinear;
    }
    
    // Create new audio buffer
    const newBuffer = audioBuffer.context.createBuffer(
      audioBuffer.numberOfChannels,
      audioBuffer.length,
      audioBuffer.sampleRate
    );
    
    newBuffer.copyToChannel(normalizedData, 0);
    
    return newBuffer;
  }
  
  private analyzeLUFS(channelData: Float32Array): number {
    // K-weighting filter (simplified)
    const kWeighted = this.applyKWeighting(channelData);
    
    // RMS calculation
    const rms = this.calculateRMS(kWeighted);
    
    // LUFS conversion
    return this.rmsToLUFS(rms);
  }
  
  private applyKWeighting(channelData: Float32Array): Float32Array {
    // Simplified K-weighting filter
    // In a full implementation, this would be a proper IIR filter
    const kWeighted = new Float32Array(channelData.length);
    
    for (let i = 0; i < channelData.length; i++) {
      kWeighted[i] = channelData[i] * 0.8; // Simplified weighting
    }
    
    return kWeighted;
  }
  
  private calculateRMS(channelData: Float32Array): number {
    const sum = channelData.reduce((acc, sample) => acc + sample * sample, 0);
    return Math.sqrt(sum / channelData.length);
  }
  
  private rmsToLUFS(rms: number): number {
    // Simplified LUFS calculation
    return 20 * Math.log10(rms) - 0.691;
  }
}
```

---

## ‚öôÔ∏è **QUALITY MANAGEMENT**

### **Quality Manager**

```typescript
// Quality Manager
interface QualitySettings {
  bitrate: number;
  format: string;
  lufs: number;
}

class QualityManager {
  private qualityLevels = {
    low: { bitrate: 128, format: 'aac', lufs: -14.0 },
    normal: { bitrate: 256, format: 'aac', lufs: -14.0 },
    high: { bitrate: 320, format: 'aac', lufs: -14.0 },
    lossless: { bitrate: 0, format: 'flac', lufs: -14.0 }
  };
  
  private currentQuality: QualityLevel = 'normal';
  
  setQuality(quality: QualityLevel): void {
    this.currentQuality = quality;
    this.applyQualitySettings();
  }
  
  getQuality(): QualitySettings {
    return this.qualityLevels[this.currentQuality];
  }
  
  private applyQualitySettings(): void {
    const settings = this.getQuality();
    
    // Apply quality settings to audio processing
    // This would integrate with the audio processing pipeline
    console.log('Applied quality settings:', settings);
  }
}
```

---

## üé® **UI INTEGRATION**

### **Enhanced Player Component**

```typescript
// Enhanced Player Component
interface EnhancedPlayerProps {
  src: string;
  title: string;
  showDownload?: boolean;
  clamp?: number;
  quality?: QualityLevel;
}

const EnhancedPlayer: React.FC<EnhancedPlayerProps> = ({
  src,
  title,
  showDownload = false,
  clamp = 30,
  quality = 'normal'
}) => {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const playerRef = useRef<WebAudioPlayer | null>(null);
  
  const [isPlaying, setIsPlaying] = useState(false);
  const [position, setPosition] = useState(0);
  const [duration, setDuration] = useState(0);
  const [volume, setVolume] = useState(1);
  
  useEffect(() => {
    if (canvasRef.current) {
      playerRef.current = new WebAudioPlayer(canvasRef.current);
      playerRef.current.loadAudio(src);
    }
    
    return () => {
      if (playerRef.current) {
        playerRef.current.pause();
      }
    };
  }, [src]);
  
  const handlePlayPause = () => {
    if (!playerRef.current) return;
    
    if (isPlaying) {
      playerRef.current.pause();
      setIsPlaying(false);
    } else {
      playerRef.current.play();
      setIsPlaying(true);
    }
  };
  
  const handleVolumeChange = (newVolume: number) => {
    setVolume(newVolume);
    if (playerRef.current) {
      playerRef.current.setVolume(newVolume);
    }
  };
  
  const handleQualityChange = (newQuality: QualityLevel) => {
    if (playerRef.current) {
      playerRef.current.setQuality(newQuality);
    }
  };
  
  return (
    <div className="audio-player">
      {/* Waveform Visualization */}
      <div className="waveform-container">
        <canvas
          ref={canvasRef}
          width={800}
          height={100}
          className="waveform-canvas"
        />
      </div>
      
      {/* Player Controls */}
      <div className="player-controls">
        <button onClick={handlePlayPause}>
          {isPlaying ? 'Pause' : 'Play'}
        </button>
        
        <input
          type="range"
          min="0"
          max="1"
          step="0.01"
          value={volume}
          onChange={(e) => handleVolumeChange(parseFloat(e.target.value))}
        />
        
        <select onChange={(e) => handleQualityChange(e.target.value as QualityLevel)}>
          <option value="low">Low (128 kbps)</option>
          <option value="normal">Normal (256 kbps)</option>
          <option value="high">High (320 kbps)</option>
          <option value="lossless">Lossless</option>
        </select>
      </div>
    </div>
  );
};
```

---

## üöÄ **IMPLEMENTATION ROADMAP**

### **Week 1: Core Implementation**
- [ ] **Day 1-2**: Web Audio API integration
- [ ] **Day 3-4**: Canvas waveform visualization
- [ ] **Day 5**: LUFS normalization
- [ ] **Day 6-7**: Quality management

### **Week 2: Integration & Testing**
- [ ] **Day 1-2**: UI component integration
- [ ] **Day 3-4**: E2E testing
- [ ] **Day 5**: Performance optimization
- [ ] **Day 6-7**: Cross-browser testing

### **Week 3: Polish & Launch**
- [ ] **Day 1-2**: UI polish and animations
- [ ] **Day 3-4**: Final testing and bug fixes
- [ ] **Day 5**: Documentation
- [ ] **Day 6-7**: Launch preparation

---

## üìä **SUCCESS METRICS**

### **Performance Targets**
- **Audio Latency**: < 100ms
- **Waveform Rendering**: 60fps
- **CPU Usage**: < 5% idle
- **Memory Usage**: < 50MB

### **Quality Targets**
- **LUFS Consistency**: ¬±0.5 dB
- **Audio Quality**: Professional standards
- **Visual Quality**: Smooth waveform rendering
- **User Experience**: Intuitive controls

### **Compatibility Targets**
- **Chrome**: 90+
- **Firefox**: 88+
- **Safari**: 14+
- **Edge**: 90+

---

**Last Updated:** 2025-10-23  
**Status:** üî¥ **CRITICAL** - Web audio player enhancement  
**Timeline:** 2-3 weeks for complete implementation  
**Priority:** Waveform visualization and LUFS normalization

---

*This document defines the complete web audio player enhancement for the hackathon timeline.*
